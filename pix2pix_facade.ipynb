{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 0.4.1\n",
      "torchvision version: 0.2.1\n",
      "Is GPU available: True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "print('PyTorch version:', torch.__version__)\n",
    "print('torchvision version:', torchvision.__version__)\n",
    "use_gpu = torch.cuda.is_available() # gpuを利用できるか？\n",
    "print('Is GPU available:', use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# 各種設定\n",
    "\n",
    "# デバイス\n",
    "device = torch.device('cuda' if use_gpu else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# バッチサイズ\n",
    "batchsize = 1\n",
    "\n",
    "# シード値の設定\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "if use_gpu:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "# 学習データが置いてあるディレクトリ\n",
    "data_dir = '../../data/facades/'\n",
    "train_data_dir = data_dir + 'train/'\n",
    "test_data_dir  = data_dir + 'test/'\n",
    "val_data_dir   = data_dir + 'val/'\n",
    "\n",
    "# 生成画像を置くディレクトリ\n",
    "output_dir = data_dir + 'generated'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "    \n",
    "# 定期的に保存する重みやオプティマイザのstate_dictなんかを置くディレクトリ\n",
    "save_dir = './save/'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ディレクトリ名を指定してデータを読み込み、TensorDatasetを作成する関数\n",
    "# label_to_facadeがTrueならラベル→画像、Falseなら画像→ラベルを学習する\n",
    "def make_TensorDataset(dir_name, label_to_facade = True):\n",
    "    data_names = os.listdir(dir_name)\n",
    "    Xs = []\n",
    "    Ys = []\n",
    "    for data_name in data_names:\n",
    "        image_np = np.array(Image.open(dir_name + data_name))\n",
    "        # 訓練データは左側が画像、右側がラベルになっている(サイズは厳密に同じ)\n",
    "        if label_to_facade:\n",
    "            X = image_np[:,  image_np.shape[1] // 2:, :]\n",
    "            Y = image_np[:, :image_np.shape[1] // 2 , :]\n",
    "        else:\n",
    "            X = image_np[:, :image_np.shape[1] // 2 , :]\n",
    "            Y = image_np[:,  image_np.shape[1] // 2:, :]\n",
    "        \n",
    "        Xs.append(X)\n",
    "        Ys.append(Y)\n",
    "    \n",
    "    # 画像の輝度値は元々0〜255になっているので、-1〜1に正規化しておく\n",
    "    Xs = (np.array(Xs) - 127.5) / 127.5 \n",
    "    Ys = (np.array(Ys) - 127.5) / 127.5\n",
    "    \n",
    "    # PyTorchの[channel, height, width]の形式に合うように次元を入れ替えておく\n",
    "    Xs = np.transpose(Xs, [0, 3, 1, 2])\n",
    "    Ys = np.transpose(Ys, [0, 3, 1, 2])\n",
    "\n",
    "    # TensorDatasetを作成\n",
    "    dataset = TensorDataset(torch.from_numpy(Xs).float(), torch.from_numpy(Ys).float())\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of training data: 400\n",
      "The number of valdation data: 100\n"
     ]
    }
   ],
   "source": [
    "# 訓練データとバリデーションデータに関して、TensorDatasetを作成して、データ数を表示する\n",
    "train_data = make_TensorDataset(train_data_dir)\n",
    "val_data = make_TensorDataset(val_data_dir)\n",
    "\n",
    "print('The number of training data:', len(train_data))\n",
    "print('The number of valdation data:', len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データローダを用意\n",
    "train_loader = DataLoader(train_data, batch_size = batchsize, shuffle = True)\n",
    "val_loader = DataLoader(val_data, batch_size = batchsize, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# 訓練データのうちの1つを表示してみる\\nXs, Ys = iter(train_loader).next()\\n\\n# 入力\\nprint('The shape of input:', Xs.size()[1:])\\nX_example = Xs[0]*0.5 + 0.5 # -1~1を表示用に0~1に戻す\\nX_example = X_example.numpy()\\nplt.imshow(np.transpose(X_example, [1, 2, 0])) # 表示用に[height, width, channel]の形式に戻す\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GUIのないマシンで実行するときはコメントアウト\n",
    "'''\n",
    "# 訓練データのうちの1つを表示してみる\n",
    "Xs, Ys = iter(train_loader).next()\n",
    "\n",
    "# 入力\n",
    "print('The shape of input:', Xs.size()[1:])\n",
    "X_example = Xs[0]*0.5 + 0.5 # -1~1を表示用に0~1に戻す\n",
    "X_example = X_example.numpy()\n",
    "plt.imshow(np.transpose(X_example, [1, 2, 0])) # 表示用に[height, width, channel]の形式に戻す\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# ラベル\\nprint('The shape of label:', Ys.size()[1:])\\nY_example = Ys[0]*0.5 + 0.5\\nY_example = Y_example.numpy()\\nplt.imshow(np.transpose(Y_example, [1, 2, 0]))\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GUIのないマシンで実行するときはコメントアウト\n",
    "'''\n",
    "# ラベル\n",
    "print('The shape of label:', Ys.size()[1:])\n",
    "Y_example = Ys[0]*0.5 + 0.5\n",
    "Y_example = Y_example.numpy()\n",
    "plt.imshow(np.transpose(Y_example, [1, 2, 0]))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 便宜上のパーツを定義\n",
    "# 論文のappendixでCk/CDkとして言及されているもののうち、encoderとdiscriminatorで使うダウンサンプリングの部分\n",
    "# use_batchnormでバッチノルムを適用するか、use_dropoutでドロップアウト(0.5)を適用するかを指定できる\n",
    "# 畳み込み→バッチノルム→ドロップアウト→leakyreluの順\n",
    "# 活性化関数は0.2のleakyrelu\n",
    "class CDk_downsample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, use_batchnorm = True, use_dropout = False):\n",
    "        super(CDk_downsample, self).__init__()\n",
    "        self.cv = nn.Conv2d(in_channels, out_channels, kernel_size = 4, stride = 2, padding = 1)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.dr = nn.Dropout(0.5)\n",
    "        self.rl = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        self.use_batchnorm = use_batchnorm\n",
    "        self.use_dropout = use_dropout\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.cv(x)\n",
    "        \n",
    "        if self.use_batchnorm:\n",
    "            out = self.bn(out)\n",
    "        \n",
    "        if self.use_dropout:\n",
    "            out = self.dr(out)\n",
    "        \n",
    "        out = self.rl(out)\n",
    "        return out\n",
    "    \n",
    "# 論文のappendixでCk/CDkとして言及されているもののうち、decoderで使うアップサンプリングの部分\n",
    "# use_batchnormでバッチノルムを適用するか、use_dropoutでドロップアウト(0.5)を適用するかを指定できる\n",
    "# 畳み込み→バッチノルム→ドロップアウト→reluの順\n",
    "# 活性化関数はrelu\n",
    "class CDk_upsample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, use_batchnorm = True, use_dropout = False):\n",
    "        super(CDk_upsample, self).__init__()\n",
    "        self.tc = nn.ConvTranspose2d(in_channels, out_channels, kernel_size = 4, stride = 2, padding = 1)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.dr = nn.Dropout(0.5)\n",
    "        self.rl = nn.ReLU()\n",
    "        \n",
    "        self.use_batchnorm = use_batchnorm\n",
    "        self.use_dropout = use_dropout\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.tc(x)\n",
    "        \n",
    "        if self.use_batchnorm:\n",
    "            out = self.bn(out)\n",
    "        \n",
    "        if self.use_dropout:\n",
    "            out = self.dr(out)\n",
    "            \n",
    "        out = self.rl(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pix2pixのGeneratorを定義\n",
    "# U-net Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.encoder_C1  = CDk_downsample(  3,  64, use_batchnorm = False)\n",
    "        self.encoder_C2  = CDk_downsample( 64, 128)\n",
    "        self.encoder_C3  = CDk_downsample(128, 256)\n",
    "        self.encoder_C4  = CDk_downsample(256, 512)\n",
    "        self.encoder_C5  = CDk_downsample(512, 512)\n",
    "        self.encoder_C6  = CDk_downsample(512, 512)\n",
    "        self.encoder_C7  = CDk_downsample(512, 512)\n",
    "        self.encoder_C8  = CDk_downsample(512, 512, use_batchnorm = False)\n",
    "        \n",
    "        self.decoder_CD1 = CDk_upsample( 512    ,  512, use_batchnorm = True, use_dropout = True)\n",
    "        self.decoder_CD2 = CDk_upsample(1024    , 1024, use_batchnorm = True, use_dropout = True)\n",
    "        self.decoder_CD3 = CDk_upsample(1024+512, 1024, use_batchnorm = True, use_dropout = True)\n",
    "        self.decoder_C4  = CDk_upsample(1024+512, 1024)\n",
    "        self.decoder_C5  = CDk_upsample(1024+512, 1024)\n",
    "        self.decoder_C6  = CDk_upsample(1024+256,  512)\n",
    "        self.decoder_C7  = CDk_upsample( 512+128,  256)\n",
    "        self.decoder_C8  = CDk_upsample( 256+ 64,  128)\n",
    "        \n",
    "        self.decoder_final = nn.Conv2d(128, 3, kernel_size = 3, stride = 1, padding = 1)\n",
    "        self.th = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out_encoder_C1  = self.encoder_C1(x)\n",
    "        out_encoder_C2  = self.encoder_C2(out_encoder_C1)\n",
    "        out_encoder_C3  = self.encoder_C3(out_encoder_C2)\n",
    "        out_encoder_C4  = self.encoder_C4(out_encoder_C3)\n",
    "        out_encoder_C5  = self.encoder_C5(out_encoder_C4)\n",
    "        out_encoder_C6  = self.encoder_C6(out_encoder_C5)\n",
    "        out_encoder_C7  = self.encoder_C7(out_encoder_C6)\n",
    "        out_encoder_C8  = self.encoder_C8(out_encoder_C7)\n",
    "        \n",
    "        out_decoder_CD1 = self.decoder_CD1(out_encoder_C8)\n",
    "        out_decoder_CD2 = self.decoder_CD2(torch.cat([out_decoder_CD1, out_encoder_C7], dim=1))\n",
    "        out_decoder_CD3 = self.decoder_CD3(torch.cat([out_decoder_CD2, out_encoder_C6], dim=1))\n",
    "        out_decoder_C4  = self.decoder_C4( torch.cat([out_decoder_CD3, out_encoder_C5], dim=1))\n",
    "        out_decoder_C5  = self.decoder_C5( torch.cat([out_decoder_C4 , out_encoder_C4], dim=1))\n",
    "        out_decoder_C6  = self.decoder_C6( torch.cat([out_decoder_C5 , out_encoder_C3], dim=1))\n",
    "        out_decoder_C7  = self.decoder_C7( torch.cat([out_decoder_C6 , out_encoder_C2], dim=1))\n",
    "        out_decoder_C8  = self.decoder_C8( torch.cat([out_decoder_C7 , out_encoder_C1], dim=1))\n",
    "        \n",
    "        out = self.decoder_final(out_decoder_C8)\n",
    "        out = self.th(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pix2pixのdiscriminatorを定義\n",
    "# patchGAN discriminator（パッチサイズがどうなってるのかよくわからん）\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.C1_x = CDk_downsample(  3,  64, use_batchnorm = False)\n",
    "        self.C1_y = CDk_downsample(  3,  64, use_batchnorm = False)\n",
    "        self.C2   = CDk_downsample(128, 128)\n",
    "        self.C3   = CDk_downsample(128, 256)\n",
    "        self.C4   = CDk_downsample(256, 512)\n",
    "        self.conv_final = nn.Conv2d(512, 1, kernel_size = 4, stride = 2, padding = 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        out_x = self.C1_x(x)\n",
    "        out_y = self.C1_y(y)\n",
    "        out = torch.cat([out_x, out_y], dim=1)\n",
    "        out = self.C2(out)\n",
    "        out = self.C3(out)\n",
    "        out = self.C4(out)\n",
    "        out = self.conv_final(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nprint('--------------------------------------------------------------------------------------')\\nprint('Generator')\\nprint('The number of trainable parameters:', num_trainable_params_gen)\\nprint('\\nModel\\n', generator)\\nprint('\\nOptimizer\\n', gen_optimizer)\\n\\nprint('--------------------------------------------------------------------------------------')\\nprint('Discriminator')\\nprint('The number of trainable parameters:', num_trainable_params_dis)\\nprint('\\nModel\\n', discriminator)\\nprint('\\nOptimizer\\n', dis_optimizer)\\n\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ネットワークを実体化、オプティマイザを定義\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "generator = generator.to(device)\n",
    "discriminator = discriminator.to(device)\n",
    "\n",
    "gen_optimizer = optim.Adam(generator.parameters(), lr = 0.0002, betas = [0.5, 0.999])\n",
    "dis_optimizer = optim.Adam(discriminator.parameters(), lr = 0.0002, betas = [0.5, 0.999])\n",
    "\n",
    "num_trainable_params_gen = sum(p.numel() for p in generator.parameters() if p.requires_grad)\n",
    "num_trainable_params_dis = sum(p.numel() for p in discriminator.parameters() if p.requires_grad)\n",
    "\n",
    "for p in generator.parameters():\n",
    "    nn.init.normal_(p, mean = 0, std = 0.02)\n",
    "    \n",
    "for p in discriminator.parameters():\n",
    "    nn.init.normal_(p, mean = 0, std = 0.02)\n",
    "'''\n",
    "print('--------------------------------------------------------------------------------------')\n",
    "print('Generator')\n",
    "print('The number of trainable parameters:', num_trainable_params_gen)\n",
    "print('\\nModel\\n', generator)\n",
    "print('\\nOptimizer\\n', gen_optimizer)\n",
    "\n",
    "print('--------------------------------------------------------------------------------------')\n",
    "print('Discriminator')\n",
    "print('The number of trainable parameters:', num_trainable_params_dis)\n",
    "print('\\nModel\\n', discriminator)\n",
    "print('\\nOptimizer\\n', dis_optimizer)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_loss(gen_output, dis_output_fake, input_label):\n",
    "    λ = 100\n",
    "    adversarial_loss = F.binary_cross_entropy(dis_output_fake, torch.ones_like(dis_output_fake))\n",
    "    l1_loss = F.l1_loss(gen_output, input_label)\n",
    "    return adversarial_loss + λ*l1_loss\n",
    "    \n",
    "def dis_loss(dis_output_real, dis_output_fake):\n",
    "    real_loss = F.binary_cross_entropy(dis_output_real, torch.ones_like(dis_output_real))\n",
    "    fake_loss = F.binary_cross_entropy(dis_output_fake, torch.zeros_like(dis_output_fake))\n",
    "    adversarial_loss =  0.5 * (real_loss + fake_loss)\n",
    "    return adversarial_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loader):\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    \n",
    "    running_generator_loss = 0\n",
    "    running_discriminator_loss = 0\n",
    "    \n",
    "    for inputs, labels in data_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        gen_outputs = generator(inputs)\n",
    "        dis_outputs_real = discriminator(labels, inputs)\n",
    "        dis_outputs_fake_for_dis = discriminator(gen_outputs.detach(), inputs)\n",
    "        dis_outputs_fake_for_gen = discriminator(gen_outputs, inputs)\n",
    "        \n",
    "        dis_optimizer.zero_grad()\n",
    "        discriminator_loss = dis_loss(dis_outputs_real, dis_outputs_fake_for_dis)\n",
    "        discriminator_loss.backward()\n",
    "        dis_optimizer.step()\n",
    "        \n",
    "        gen_optimizer.zero_grad()\n",
    "        generator_loss = gen_loss(gen_outputs, dis_outputs_fake_for_gen, labels)\n",
    "        generator_loss.backward()\n",
    "        gen_optimizer.step()\n",
    "                                                \n",
    "        running_generator_loss += generator_loss.item()\n",
    "        running_discriminator_loss += discriminator_loss.item()\n",
    "                                                \n",
    "    average_generator_loss = running_generator_loss / len(data_loader)\n",
    "    average_discriminator_loss = running_discriminator_loss / len(data_loader)\n",
    "    \n",
    "    return average_generator_loss, average_discriminator_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(data_loader, epoch):\n",
    "    n_save_images = 8\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    \n",
    "    running_generator_loss = 0\n",
    "    running_discriminator_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(data_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            gen_outputs = generator(inputs)\n",
    "            dis_outputs_real = discriminator(labels, inputs)\n",
    "            dis_outputs_fake = discriminator(gen_outputs, inputs)\n",
    "       \n",
    "            running_discriminator_loss += dis_loss(dis_outputs_real, dis_outputs_fake).item()\n",
    "            running_generator_loss += gen_loss(gen_outputs, dis_outputs_fake, labels).item()\n",
    "            \n",
    "            if epoch % 2 == 0:\n",
    "                if i >= len(data_loader) - n_save_images:\n",
    "                    comparison = torch.cat([inputs, labels, gen_outputs])\n",
    "                    save_image(comparison.data.cpu(), '{}/{}_{}.png'.format(output_dir, epoch, i))\n",
    "                \n",
    "    average_generator_loss = running_generator_loss / len(data_loader)\n",
    "    average_discriminator_loss = running_discriminator_loss / len(data_loader)\n",
    "\n",
    "    return average_generator_loss, average_discriminator_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[1/20] losses[train_gen:38.5265 train_dis:0.2463  val_gen:38.0373 val_dis:0.0352]\n",
      "epoch[2/20] losses[train_gen:39.3692 train_dis:0.2150  val_gen:38.3708 val_dis:0.2278]\n",
      "epoch[3/20] losses[train_gen:39.5282 train_dis:0.2359  val_gen:40.5113 val_dis:0.2546]\n",
      "epoch[4/20] losses[train_gen:39.8532 train_dis:0.2340  val_gen:37.5642 val_dis:0.2156]\n",
      "epoch[5/20] losses[train_gen:39.7802 train_dis:0.2482  val_gen:39.6500 val_dis:0.2275]\n",
      "epoch[6/20] losses[train_gen:39.7290 train_dis:0.2410  val_gen:38.6080 val_dis:0.1959]\n",
      "epoch[7/20] losses[train_gen:38.9951 train_dis:0.2488  val_gen:39.3466 val_dis:0.2096]\n",
      "epoch[8/20] losses[train_gen:38.8680 train_dis:0.2622  val_gen:38.7222 val_dis:0.1543]\n",
      "epoch[9/20] losses[train_gen:38.0801 train_dis:0.2546  val_gen:37.9255 val_dis:0.1576]\n",
      "epoch[10/20] losses[train_gen:37.6144 train_dis:0.2594  val_gen:38.5444 val_dis:0.7065]\n",
      "epoch[11/20] losses[train_gen:36.7984 train_dis:0.2751  val_gen:36.9270 val_dis:0.2636]\n",
      "epoch[12/20] losses[train_gen:35.8562 train_dis:0.2796  val_gen:39.0482 val_dis:0.2355]\n",
      "epoch[13/20] losses[train_gen:34.8814 train_dis:0.3010  val_gen:37.9060 val_dis:0.3553]\n",
      "epoch[14/20] losses[train_gen:34.0791 train_dis:0.3182  val_gen:39.6580 val_dis:0.2172]\n",
      "epoch[15/20] losses[train_gen:33.0780 train_dis:0.3233  val_gen:39.8025 val_dis:0.3995]\n",
      "epoch[16/20] losses[train_gen:32.1524 train_dis:0.3400  val_gen:39.7810 val_dis:0.4626]\n",
      "epoch[17/20] losses[train_gen:30.9409 train_dis:0.3692  val_gen:37.4578 val_dis:0.2405]\n",
      "epoch[18/20] losses[train_gen:30.3335 train_dis:0.3662  val_gen:37.6635 val_dis:0.9086]\n",
      "epoch[19/20] losses[train_gen:29.3369 train_dis:0.3745  val_gen:40.8002 val_dis:0.5645]\n",
      "epoch[20/20] losses[train_gen:28.5608 train_dis:0.3849  val_gen:38.7524 val_dis:0.4177]\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "train_loss_list = [[],[]]\n",
    "val_loss_list = [[],[]]\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_gen_loss, train_dis_loss = train(train_loader)\n",
    "    val_gen_loss, val_dis_loss = val(val_loader, epoch)\n",
    "    \n",
    "    train_loss_list[0].append(train_gen_loss)\n",
    "    train_loss_list[1].append(train_dis_loss)\n",
    "    val_loss_list[0].append(val_gen_loss)\n",
    "    val_loss_list[1].append(val_dis_loss)\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        torch.save(generator.state_dict(), save_dir + 'generator_' + str(epoch) + '.pth')\n",
    "        torch.save(discriminator.state_dict(), save_dir + 'discriminator_' + str(epoch) + '.pth')\n",
    "        torch.save(gen_optimizer.state_dict(), save_dir + 'gen_optmizer_' + str(epoch) + '.pth')\n",
    "        torch.save(dis_optimizer.state_dict(), save_dir + 'gen_discriminator' + str(epoch) + '.pth')\n",
    "    \n",
    "    print('epoch[%d/%d] losses[train_gen:%1.4f train_dis:%1.4f  val_gen:%1.4f val_dis:%1.4f]' \\\n",
    "                    % (epoch+1, n_epochs, train_gen_loss, train_dis_loss, val_gen_loss, val_dis_loss))\n",
    "    \n",
    "np.save(save_dir +  'train_loss_list.npy', np.array(train_loss_list))\n",
    "np.save(save_dir + 'validation_loss_list.npy', np.array(val_loss_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
